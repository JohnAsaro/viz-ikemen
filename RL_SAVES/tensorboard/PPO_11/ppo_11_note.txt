Ai level 1, learning rate 0.0001, clip_range 0.2, other than that the same 

    n_steps = 32768 # Number of steps to take before revaluting the policy
    env = IkemenEnv(ai_level=2, screen_width=80, screen_height=60, show_capture=False, n_steps=n_steps, showcase=False, step_delay = 0.00555555555, headless = False, speed = 24, fps = 180)  # Create the Ikemen environment


    verbose = 1 # Verbosity level for the model training
    learning_rate = 0.0001 # Learning rate for the PPO model
    batch_size = 64 # Batch size for the PPO model
    n_epochs = 5 # Number of epochs for the PPO model
    gamma = 1.0 # Discount factor for the PPO model, since no reward shaping, 1.0 because just win/lose
    gae_lambda = 1.0 # GAE lambda for the PPO model, sparse reward so 1.0
    clip_range = 0.2 # Clipping range for the PPO model
    device = "cpu" # PPO works well on cpu, but can be changed to "cuda" for GPU training
    tensorboard_log=os.path.join(RL_SAVES, "tensorboard") # Tensorboard log path

3m steps

(Cont ppo run in 12, because ppo 11 started falling off at 300000 steps, so im starting from there)

WILL COMBINE LOGS

PPO 11 = BATCHES 1-9

ppo 12:
train_PPO(env, timesteps=3000000, check=100000, num_steps=n_steps, model_path=os.path.join(RL_SAVES, "models", "PPO_11", "best_model_300000.zip"))  # Train the PPO model

PPO 12 = BATCHES 1-6, IN LOG WILL BE BATCHES 10-15

STOPPED AT 200000, for PPO 13 SAVE EVERY ITERATION TO PREVENT AKWARD FALLOFFS

PPO 13: 
train_PPO(env, timesteps=1000000, check=32768, num_steps=n_steps, model_path=os.path.join(RL_SAVES, "models", "PPO_12", "best_model_200000.zip"))  # Train the PPO model

PPO 13 = BATCHES 1-5, IN LOG WILL BE BATCHES 16-20

BEST GEN AT 131072/batch 4

PPO 14:

REDUCED CLIP RANGE TO 0.1

train_PPO(env, timesteps=1000000, check=32768, num_steps=n_steps, model_path=os.path.join(RL_SAVES, "models", "PPO_13", "best_model_131072.zip"))  # Train the PPO model

ran to 524288.